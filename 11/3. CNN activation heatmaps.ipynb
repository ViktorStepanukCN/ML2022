{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22544c9",
   "metadata": {},
   "source": [
    "# Class activation map\n",
    "\n",
    "CNNs are little bit less black boxy than FCN.\n",
    "\n",
    "One of the reason is ability to help developers explain why the neural network made some decision using technique called Class Activation Map (CAM).\n",
    "\n",
    "It could help us debug problems with predictions becasue it allows us to see what exact part of images played the biggest role for making decisions.\n",
    "\n",
    "It produces 2D grid of scores associated with specific output class computed for every location of the input image.\n",
    "\n",
    "The easiest CAM can be obtained from CNN architectures working with GlobalAveragePooling (ResNet, Xception, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d46b7",
   "metadata": {},
   "source": [
    "Load ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb34302",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = ResNet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447de1f",
   "metadata": {},
   "source": [
    "Load and preprocess image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363664b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('dog.jpg')\n",
    "img = cv2.resize(img, (224,224))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(img, axis=0)\n",
    "X = tf.keras.applications.resnet50.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa71733",
   "metadata": {},
   "source": [
    "Select the last convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in res_model.layers:\n",
    "    print(l.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b5a798",
   "metadata": {},
   "source": [
    "Select last convolutional layer output and prediction as outputs from the CAM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = res_model.get_layer(\"conv5_block3_out\").output\n",
    "pred_ouptut = res_model.get_layer(\"predictions\").output\n",
    "model = Model(res_model.input, outputs=[conv_output, pred_ouptut])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae149eb5",
   "metadata": {},
   "source": [
    "Make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64570f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features, results = model.predict(X)\n",
    "tf.keras.applications.resnet50.decode_predictions(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defef78c",
   "metadata": {},
   "source": [
    "Last convolutional layer before GlobalAveragePooling creates 2048 7*7 feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4354ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48833780",
   "metadata": {},
   "source": [
    "### Class activations\n",
    "To generate class activation map we want to see which image feature are the most important to for generating output probabilities.\n",
    "\n",
    "Each feature filters is tailored to look for a specific set of features - these are learned during the training.\n",
    "\n",
    "Dense layer decides how much weight to give for each of features for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69623e75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(36):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    plt.imshow(img)\n",
    "    heatmap = cv2.resize(features[0, :,:,i], (img.shape[1], img.shape[0]))\n",
    "    plt.imshow(heatmap, cmap='jet', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23663d89",
   "metadata": {},
   "source": [
    "Global average pooling collapses output feature maps of the last CNN to a single value features that go to the dense layer for prediction.\n",
    "\n",
    "The dense layer assigns weights to each of those features (for each of 1000 classes in this case).\n",
    "\n",
    "So we need to get weights of the last dense layer and compute dot product with features from the last CNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights from the last Dense layer\n",
    "w = model.get_layer(\"predictions\").weights[0]\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0507c",
   "metadata": {},
   "source": [
    "CNN features for the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_img = features[0, :,:,:]\n",
    "features_for_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d86c9",
   "metadata": {},
   "source": [
    "Scale are just 7*7, so scale them up to match dimensions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0159dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features_for_img_scaled = zoom(features_for_img, (224/7, 224/7, 1), order=2)\n",
    "features_for_img_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb252a13",
   "metadata": {},
   "source": [
    "Select weights used for predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590755",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.argmax(results, axis=1).squeeze()\n",
    "print(target)\n",
    "weights = w[:, target]\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d75177",
   "metadata": {},
   "source": [
    "Calculate class activation map as the dot product of the scaled convolution features and weights for one class.\n",
    "\n",
    "Dot product results in a scalar value at each pixel.\n",
    "\n",
    "The resulting scalar result will be larger when the image both has the particular feature, and that feature is also weighted more heavily for the particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae175fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = np.dot(features_for_img_scaled, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964024e8",
   "metadata": {},
   "source": [
    "Show class activation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95dc77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(img)\n",
    "plt.imshow(cam, cmap='jet', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dee72b",
   "metadata": {},
   "source": [
    "## Grad-CAM\n",
    "\n",
    "More generalized solution for creating activation maps using gradients.\n",
    "\n",
    "https://arxiv.org/abs/1610.02391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG16(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d8f0f",
   "metadata": {},
   "source": [
    "VGG16 has multiple classifier layers and do not contain Global Average Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb95ba3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(vgg_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in vgg_model.layers:\n",
    "    print(l.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b8cb8",
   "metadata": {},
   "source": [
    "Take the last convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = 'block5_conv3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43675a8",
   "metadata": {},
   "source": [
    "Process the image for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe89ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('elephant.jpg')\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25184ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(img, axis=0)\n",
    "X = tf.keras.applications.vgg16.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abf39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vgg_model.predict(X)\n",
    "tf.keras.applications.vgg16.decode_predictions(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94ba32",
   "metadata": {},
   "source": [
    "Again, we need to create a model that maps image to the activations of the last conv layer and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = vgg_model.get_layer(conv_output).output\n",
    "pred_ouptut = vgg_model.get_layer('predictions').output\n",
    "grad_model = Model(vgg_model.input, outputs=[conv_output, pred_ouptut])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8fa9cf",
   "metadata": {},
   "source": [
    "Computing gradient of the predicted class with respect to the activations of the last convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c509e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer_output, preds = grad_model(X)\n",
    "    pred_index = tf.argmax(preds[0])\n",
    "    class_channel = preds[:, pred_index]\n",
    "# gradient of the output neuron with regard to the output feature map of the last conv layer\n",
    "grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "print(f'gradient shape: {grads.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf69e80",
   "metadata": {},
   "source": [
    "Get average intesity of the gradient of each of feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "print(f'pooled gradients shape: {pooled_grads.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47433dc2",
   "metadata": {},
   "source": [
    "Weight the convolution outputs with the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49828a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_output = np.squeeze(last_conv_layer_output.numpy())\n",
    "for i in range(pooled_grads.shape[-1]):\n",
    "    last_conv_layer_output[:, :, i] *= pooled_grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9b854",
   "metadata": {},
   "source": [
    "Average all feature channels to a one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa591ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.mean(last_conv_layer_output, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f61b7",
   "metadata": {},
   "source": [
    "Normalize between 0..1 for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafadb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(img)\n",
    "cam = cv2.resize(heatmap.numpy(), (img.shape[1], img.shape[0]))\n",
    "plt.imshow(cam, cmap='jet', alpha=0.3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dad35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
